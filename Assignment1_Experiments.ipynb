{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3f19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b73a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure directories\n",
    "os.makedirs(\"history\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb389756",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3481158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# TEXT and LABEL fields\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "                  tokenizer_language='en_core_web_sm',\n",
    "                  include_lengths=True,\n",
    "                  pad_first=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f3c07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "# split train -> train/valid\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
    "\n",
    "# build vocab\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# iterators\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24550d00",
   "metadata": {},
   "source": [
    "### Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b630117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fab0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN (simple)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        # text: [sent_len, batch]\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        return self.fc(hidden.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38be84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward (pooled embeddings)\n",
    "class FeedForwardText(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dims, output_dim, dropout=0.5, pool_fn=lambda emb: emb.mean(dim=0)):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.pool_fn = pool_fn\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers = []\n",
    "        in_dim = embedding_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            in_dim = h\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "        self.fc_out = nn.Linear(in_dim, output_dim)\n",
    "        self.activation = nn.functional.relu\n",
    "\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        embedded = self.embedding(text)\n",
    "        x = self.pool_fn(embedded)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d571c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNText (three filter sizes)\n",
    "class CNNText(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, n_filters, filter_sizes=(1,2,3), output_dim=1, dropout=0.5, padding_idx=None):\n",
    "        super().__init__()\n",
    "        if isinstance(n_filters, int):\n",
    "            n_filters = (n_filters, n_filters, n_filters)\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=padding_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters[i], kernel_size=fs, padding=fs-1)\n",
    "            for i, fs in enumerate(filter_sizes)\n",
    "        ])\n",
    "        self.fc = nn.Linear(sum(n_filters), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        embedded = self.embedding(text)\n",
    "        x = embedded.permute(1, 2, 0)\n",
    "        conv_results = []\n",
    "        for conv in self.convs:\n",
    "            c = conv(x)\n",
    "            c = nn.functional.relu(c)\n",
    "            pooled = nn.functional.max_pool1d(c, kernel_size=c.size(2)).squeeze(2)\n",
    "            conv_results.append(pooled)\n",
    "        cat = torch.cat(conv_results, dim=1)\n",
    "        cat = self.dropout(cat)\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebea37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM single-layer\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        last_hidden = hidden[-1]\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "        return self.fc(last_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2310ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        forward_hidden = hidden[-2]\n",
    "        backward_hidden = hidden[-1]\n",
    "        cat_hidden = torch.cat((forward_hidden, backward_hidden), dim=1)\n",
    "        cat_hidden = self.dropout(cat_hidden)\n",
    "        return self.fc(cat_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1248f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(INPUT_DIM, EMBEDDING_DIM=100, HIDDEN_DIM=256, OUTPUT_DIM=1):\n",
    "    models = {}\n",
    "    models['ff1'] = FeedForwardText(INPUT_DIM, EMBEDDING_DIM, hidden_dims=[500], output_dim=OUTPUT_DIM)\n",
    "    models['ff2'] = FeedForwardText(INPUT_DIM, EMBEDDING_DIM, hidden_dims=[500,300], output_dim=OUTPUT_DIM)\n",
    "    models['ff3'] = FeedForwardText(INPUT_DIM, EMBEDDING_DIM, hidden_dims=[500,300,200], output_dim=OUTPUT_DIM)\n",
    "    models['cnn'] = CNNText(INPUT_DIM, EMBEDDING_DIM, n_filters=100, filter_sizes=(1,2,3), output_dim=OUTPUT_DIM, padding_idx=None)\n",
    "    models['lstm'] = LSTMModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, n_layers=1, dropout=0.5)\n",
    "    models['bilstm'] = BiLSTMModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, n_layers=1, dropout=0.5)\n",
    "    models['rnn'] = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80f8af",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "`device iterator`: Move batches to gpu, and keep sequence lengths in cpu. Sequence lengths are just integers describing how long each sequence is. They don’t require gradients or backpropagation, so there’s no computational benefit to putting them on the GPU.\n",
    "\n",
    "`binary accuracy`: (correct predictions) / (total predictions)\n",
    "\n",
    "`train`: train the model, by passing it batches, recording the loss and backpropogating it.\n",
    "\n",
    "`evaluate`: similar loop as train, but does not backpropogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07124d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_iterator(iterator, device):\n",
    "    for batch in iterator:\n",
    "        try:\n",
    "            text, text_lengths = batch.text\n",
    "            text = text.to(device)\n",
    "            batch.text = (text, text_lengths)\n",
    "        except Exception:\n",
    "            try:\n",
    "                batch.text = batch.text.to(device)\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            batch.label = batch.label.to(device)\n",
    "        except Exception:\n",
    "            pass\n",
    "        yield batch\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    device = next(model.parameters()).device\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    batch_count = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        device_iterator(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        batch_count += 1\n",
    "    if batch_count == 0:\n",
    "        return 0.0, 0.0\n",
    "    return epoch_loss / batch_count, epoch_acc / batch_count\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    device = next(model.parameters()).device\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    batch_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            device_iterator(batch, device)\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            batch_count += 1\n",
    "    if batch_count == 0:\n",
    "        return 0.0, 0.0\n",
    "    return epoch_loss / batch_count, epoch_acc / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cfc005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dfc308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_csv_json(history, prefix):\n",
    "    if not history:\n",
    "        return\n",
    "    json_path = f\"history/{prefix}-history.json\"\n",
    "    with open(json_path, \"w\") as jf:\n",
    "        json.dump(history, jf, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57231e",
   "metadata": {},
   "source": [
    "`run and record`: run the experiments and record the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0556429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_record(model_name, model, train_iter, valid_iter, n_epochs=50, lr=1e-3, output_prefix=\"results\"):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    history = []\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train(model, device_iterator(train_iter, DEVICE), optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, device_iterator(valid_iter, DEVICE), criterion)\n",
    "        end_time = time.time()\n",
    "        mins, secs = divmod(int(end_time - start_time), 60)\n",
    "\n",
    "        # per-epoch checkpoint <commenting, cause saving the best one instead, per epoch doesnt seem relevant rn>\n",
    "        #epoch_ckpt_path = f\"models/{output_prefix}-{model_name}-epoch{epoch}.pt\"\n",
    "        #torch.save(model.state_dict(), epoch_ckpt_path)\n",
    "\n",
    "        # save best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"models/{output_prefix}-{model_name}-best.pt\")\n",
    "\n",
    "        row = OrderedDict([\n",
    "            (\"epoch\", epoch),\n",
    "            (\"train_loss\", train_loss),\n",
    "            (\"train_acc\", train_acc),\n",
    "            (\"valid_loss\", valid_loss),\n",
    "            (\"valid_acc\", valid_acc),\n",
    "            (\"time_mins\", mins),\n",
    "            (\"time_secs\", secs),\n",
    "            ('num_parameters', count_parameters(model))\n",
    "        ])\n",
    "        history.append(row)\n",
    "\n",
    "        print(f\"{model_name} | Epoch {epoch:02} | {mins}m {secs}s | Train Loss {train_loss:.4f} Train Acc {train_acc*100:.2f}% | Val Loss {valid_loss:.4f} Val Acc {valid_acc*100:.2f}%\")\n",
    "\n",
    "    # save history\n",
    "    save_history_csv_json(history, f\"{output_prefix}-{model_name}\")\n",
    "\n",
    "    return history, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bbc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc4878ee",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Warm up: Read, understand, and reimplement the example in the code base.  \n",
    "    The sample experiment provided in `Assignment_1_Simple_Sentiment_Analysis.ipynb` is reimplemented as part of the optimizer sweep.\n",
    "2. Conduct experiments with different optimizers: SGD, Adam, Adagrad, and record the experimental results \n",
    "3. Use Adam optimizer, conduct experiments with different numbers of epochs: 5, 10, 20, and 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5980142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "rnn-SGD | Epoch 01 | 0m 5s | Train Loss 0.6800 Train Acc 57.26% | Val Loss 0.6640 Val Acc 60.09%\n",
      "rnn-SGD | Epoch 02 | 0m 4s | Train Loss 0.6790 Train Acc 55.79% | Val Loss 0.6932 Val Acc 51.14%\n",
      "rnn-SGD | Epoch 03 | 0m 4s | Train Loss 0.6932 Train Acc 52.43% | Val Loss 0.6919 Val Acc 52.53%\n",
      "rnn-SGD | Epoch 04 | 0m 4s | Train Loss 0.6930 Train Acc 52.21% | Val Loss 0.6890 Val Acc 53.30%\n",
      "rnn-SGD | Epoch 05 | 0m 4s | Train Loss 0.6908 Train Acc 52.96% | Val Loss 0.6867 Val Acc 53.44%\n",
      "rnn-SGD | Epoch 06 | 0m 4s | Train Loss 0.6873 Train Acc 53.80% | Val Loss 0.6838 Val Acc 53.48%\n",
      "rnn-SGD | Epoch 07 | 0m 4s | Train Loss 0.6820 Train Acc 55.43% | Val Loss 0.6717 Val Acc 58.33%\n",
      "rnn-SGD | Epoch 08 | 0m 4s | Train Loss 0.6429 Train Acc 62.76% | Val Loss 0.6257 Val Acc 67.01%\n",
      "rnn-SGD | Epoch 09 | 0m 4s | Train Loss 0.5902 Train Acc 69.96% | Val Loss 0.6525 Val Acc 59.05%\n",
      "rnn-SGD | Epoch 10 | 0m 4s | Train Loss 0.5673 Train Acc 71.14% | Val Loss 0.6344 Val Acc 63.15%\n",
      "rnn-SGD | Epoch 11 | 0m 4s | Train Loss 0.5450 Train Acc 73.31% | Val Loss 0.6680 Val Acc 58.23%\n",
      "rnn-SGD | Epoch 12 | 0m 4s | Train Loss 0.5198 Train Acc 74.48% | Val Loss 0.6365 Val Acc 61.69%\n",
      "rnn-SGD | Epoch 13 | 0m 4s | Train Loss 0.4709 Train Acc 79.20% | Val Loss 0.5673 Val Acc 73.04%\n",
      "rnn-SGD | Epoch 14 | 0m 6s | Train Loss 0.3937 Train Acc 83.47% | Val Loss 0.5525 Val Acc 73.74%\n",
      "rnn-SGD | Epoch 15 | 0m 4s | Train Loss 0.3892 Train Acc 84.04% | Val Loss 0.7146 Val Acc 74.52%\n",
      "rnn-SGD | Epoch 16 | 0m 4s | Train Loss 0.3430 Train Acc 86.62% | Val Loss 0.5412 Val Acc 73.97%\n",
      "rnn-SGD | Epoch 17 | 0m 4s | Train Loss 0.3404 Train Acc 86.20% | Val Loss 0.5857 Val Acc 75.49%\n",
      "rnn-SGD | Epoch 18 | 0m 4s | Train Loss 0.3584 Train Acc 85.25% | Val Loss 0.5727 Val Acc 76.00%\n",
      "rnn-SGD | Epoch 19 | 0m 4s | Train Loss 0.2948 Train Acc 88.68% | Val Loss 0.5666 Val Acc 76.76%\n",
      "rnn-SGD | Epoch 20 | 0m 4s | Train Loss 0.2732 Train Acc 89.82% | Val Loss 0.5963 Val Acc 76.48%\n",
      "\n",
      "============================================================\n",
      "rnn-Adagrad | Epoch 01 | 0m 4s | Train Loss 0.6817 Train Acc 56.38% | Val Loss 0.6520 Val Acc 61.78%\n",
      "rnn-Adagrad | Epoch 02 | 0m 4s | Train Loss 0.6793 Train Acc 56.43% | Val Loss 0.6700 Val Acc 57.49%\n",
      "rnn-Adagrad | Epoch 03 | 0m 4s | Train Loss 0.6489 Train Acc 60.98% | Val Loss 0.6847 Val Acc 54.10%\n",
      "rnn-Adagrad | Epoch 04 | 0m 4s | Train Loss 0.6501 Train Acc 61.18% | Val Loss 0.6559 Val Acc 61.10%\n",
      "rnn-Adagrad | Epoch 05 | 0m 4s | Train Loss 0.5878 Train Acc 68.65% | Val Loss 0.6144 Val Acc 65.90%\n",
      "rnn-Adagrad | Epoch 06 | 0m 4s | Train Loss 0.5213 Train Acc 73.98% | Val Loss 0.5951 Val Acc 68.19%\n",
      "rnn-Adagrad | Epoch 07 | 0m 4s | Train Loss 0.4881 Train Acc 76.72% | Val Loss 0.6734 Val Acc 66.75%\n",
      "rnn-Adagrad | Epoch 08 | 0m 4s | Train Loss 0.4470 Train Acc 79.39% | Val Loss 0.6024 Val Acc 69.04%\n",
      "rnn-Adagrad | Epoch 09 | 0m 4s | Train Loss 0.3825 Train Acc 83.16% | Val Loss 0.6109 Val Acc 73.50%\n",
      "rnn-Adagrad | Epoch 10 | 0m 4s | Train Loss 0.3411 Train Acc 85.66% | Val Loss 0.6593 Val Acc 67.20%\n",
      "rnn-Adagrad | Epoch 11 | 0m 4s | Train Loss 0.5250 Train Acc 73.64% | Val Loss 0.7014 Val Acc 64.81%\n",
      "rnn-Adagrad | Epoch 12 | 0m 4s | Train Loss 0.4840 Train Acc 76.33% | Val Loss 0.6753 Val Acc 65.39%\n",
      "rnn-Adagrad | Epoch 13 | 0m 4s | Train Loss 0.3686 Train Acc 84.20% | Val Loss 0.6359 Val Acc 72.37%\n",
      "rnn-Adagrad | Epoch 14 | 0m 4s | Train Loss 0.4051 Train Acc 80.61% | Val Loss 0.6679 Val Acc 59.95%\n",
      "rnn-Adagrad | Epoch 15 | 0m 4s | Train Loss 0.4758 Train Acc 76.88% | Val Loss 0.7571 Val Acc 59.91%\n",
      "rnn-Adagrad | Epoch 16 | 0m 4s | Train Loss 0.4076 Train Acc 81.09% | Val Loss 0.7709 Val Acc 64.93%\n",
      "rnn-Adagrad | Epoch 17 | 0m 4s | Train Loss 0.3308 Train Acc 85.84% | Val Loss 0.7784 Val Acc 68.61%\n",
      "rnn-Adagrad | Epoch 18 | 0m 4s | Train Loss 0.3946 Train Acc 81.56% | Val Loss 0.7318 Val Acc 68.22%\n",
      "rnn-Adagrad | Epoch 19 | 0m 4s | Train Loss 0.2807 Train Acc 88.61% | Val Loss 0.7633 Val Acc 70.11%\n",
      "rnn-Adagrad | Epoch 20 | 0m 4s | Train Loss 0.2691 Train Acc 88.93% | Val Loss 0.7627 Val Acc 70.61%\n",
      "\n",
      "============================================================\n",
      "rnn-Adam-e5 | Epoch 01 | 0m 4s | Train Loss 0.6758 Train Acc 57.10% | Val Loss 0.6653 Val Acc 58.11%\n",
      "rnn-Adam-e5 | Epoch 02 | 0m 4s | Train Loss 0.6377 Train Acc 63.35% | Val Loss 0.6359 Val Acc 63.57%\n",
      "rnn-Adam-e5 | Epoch 03 | 0m 4s | Train Loss 0.6181 Train Acc 65.84% | Val Loss 0.6778 Val Acc 56.79%\n",
      "rnn-Adam-e5 | Epoch 04 | 0m 4s | Train Loss 0.6618 Train Acc 59.25% | Val Loss 0.6511 Val Acc 60.42%\n",
      "rnn-Adam-e5 | Epoch 05 | 0m 4s | Train Loss 0.5993 Train Acc 68.12% | Val Loss 0.5994 Val Acc 68.18%\n",
      "\n",
      "============================================================\n",
      "rnn-Adam-e10 | Epoch 01 | 0m 4s | Train Loss 0.6921 Train Acc 53.74% | Val Loss 0.6924 Val Acc 52.58%\n",
      "rnn-Adam-e10 | Epoch 02 | 0m 4s | Train Loss 0.6827 Train Acc 55.65% | Val Loss 0.6804 Val Acc 55.70%\n",
      "rnn-Adam-e10 | Epoch 03 | 0m 4s | Train Loss 0.6862 Train Acc 53.60% | Val Loss 0.7176 Val Acc 50.05%\n",
      "rnn-Adam-e10 | Epoch 04 | 0m 4s | Train Loss 0.6807 Train Acc 55.88% | Val Loss 0.6825 Val Acc 56.06%\n",
      "rnn-Adam-e10 | Epoch 05 | 0m 4s | Train Loss 0.6374 Train Acc 63.18% | Val Loss 0.6436 Val Acc 61.02%\n",
      "rnn-Adam-e10 | Epoch 06 | 0m 4s | Train Loss 0.6715 Train Acc 56.94% | Val Loss 0.6820 Val Acc 54.90%\n",
      "rnn-Adam-e10 | Epoch 07 | 0m 4s | Train Loss 0.6762 Train Acc 55.96% | Val Loss 0.7045 Val Acc 52.80%\n",
      "rnn-Adam-e10 | Epoch 08 | 0m 4s | Train Loss 0.6316 Train Acc 64.61% | Val Loss 0.6487 Val Acc 62.01%\n",
      "rnn-Adam-e10 | Epoch 09 | 0m 4s | Train Loss 0.5630 Train Acc 72.01% | Val Loss 0.6040 Val Acc 70.12%\n",
      "rnn-Adam-e10 | Epoch 10 | 0m 4s | Train Loss 0.5097 Train Acc 76.39% | Val Loss 0.6084 Val Acc 68.34%\n",
      "\n",
      "============================================================\n",
      "rnn-Adam-e20 | Epoch 01 | 0m 4s | Train Loss 0.6694 Train Acc 58.47% | Val Loss 0.7002 Val Acc 51.55%\n",
      "rnn-Adam-e20 | Epoch 02 | 0m 4s | Train Loss 0.6809 Train Acc 55.30% | Val Loss 0.7098 Val Acc 55.86%\n",
      "rnn-Adam-e20 | Epoch 03 | 0m 4s | Train Loss 0.6719 Train Acc 57.37% | Val Loss 0.6706 Val Acc 58.47%\n",
      "rnn-Adam-e20 | Epoch 04 | 0m 4s | Train Loss 0.6101 Train Acc 66.69% | Val Loss 0.6174 Val Acc 64.88%\n",
      "rnn-Adam-e20 | Epoch 05 | 0m 4s | Train Loss 0.5494 Train Acc 72.90% | Val Loss 0.6048 Val Acc 66.72%\n",
      "rnn-Adam-e20 | Epoch 06 | 0m 4s | Train Loss 0.5183 Train Acc 75.16% | Val Loss 0.5812 Val Acc 71.76%\n",
      "rnn-Adam-e20 | Epoch 07 | 0m 4s | Train Loss 0.4864 Train Acc 77.24% | Val Loss 0.5983 Val Acc 65.62%\n",
      "rnn-Adam-e20 | Epoch 08 | 0m 4s | Train Loss 0.4710 Train Acc 78.14% | Val Loss 0.5946 Val Acc 68.30%\n",
      "rnn-Adam-e20 | Epoch 09 | 0m 4s | Train Loss 0.4485 Train Acc 80.75% | Val Loss 0.5635 Val Acc 72.14%\n",
      "rnn-Adam-e20 | Epoch 10 | 0m 4s | Train Loss 0.3893 Train Acc 83.85% | Val Loss 0.5497 Val Acc 76.08%\n",
      "rnn-Adam-e20 | Epoch 11 | 0m 4s | Train Loss 0.3536 Train Acc 86.45% | Val Loss 0.5972 Val Acc 69.77%\n",
      "rnn-Adam-e20 | Epoch 12 | 0m 4s | Train Loss 0.3197 Train Acc 87.46% | Val Loss 0.5460 Val Acc 75.76%\n",
      "rnn-Adam-e20 | Epoch 13 | 0m 4s | Train Loss 0.2792 Train Acc 89.30% | Val Loss 0.6021 Val Acc 78.84%\n",
      "rnn-Adam-e20 | Epoch 14 | 0m 4s | Train Loss 0.2917 Train Acc 89.00% | Val Loss 0.5182 Val Acc 77.06%\n",
      "rnn-Adam-e20 | Epoch 15 | 0m 4s | Train Loss 0.2569 Train Acc 90.09% | Val Loss 0.5690 Val Acc 74.51%\n",
      "rnn-Adam-e20 | Epoch 16 | 0m 4s | Train Loss 0.2361 Train Acc 91.30% | Val Loss 0.5889 Val Acc 76.69%\n",
      "rnn-Adam-e20 | Epoch 17 | 0m 4s | Train Loss 0.2349 Train Acc 91.50% | Val Loss 0.6101 Val Acc 76.19%\n",
      "rnn-Adam-e20 | Epoch 18 | 0m 4s | Train Loss 0.2425 Train Acc 91.30% | Val Loss 0.6329 Val Acc 67.28%\n",
      "rnn-Adam-e20 | Epoch 19 | 0m 4s | Train Loss 0.2304 Train Acc 91.56% | Val Loss 0.6773 Val Acc 73.83%\n",
      "rnn-Adam-e20 | Epoch 20 | 0m 4s | Train Loss 0.4379 Train Acc 78.67% | Val Loss 0.6297 Val Acc 68.49%\n",
      "\n",
      "============================================================\n",
      "rnn-Adam-e50 | Epoch 01 | 0m 4s | Train Loss 0.6801 Train Acc 57.09% | Val Loss 0.6689 Val Acc 59.11%\n",
      "rnn-Adam-e50 | Epoch 02 | 0m 4s | Train Loss 0.6647 Train Acc 58.80% | Val Loss 0.6730 Val Acc 57.40%\n",
      "rnn-Adam-e50 | Epoch 03 | 0m 4s | Train Loss 0.6662 Train Acc 58.43% | Val Loss 0.6792 Val Acc 55.80%\n",
      "rnn-Adam-e50 | Epoch 04 | 0m 4s | Train Loss 0.6513 Train Acc 60.98% | Val Loss 0.6529 Val Acc 59.46%\n",
      "rnn-Adam-e50 | Epoch 05 | 0m 4s | Train Loss 0.6025 Train Acc 67.10% | Val Loss 0.6187 Val Acc 65.55%\n",
      "rnn-Adam-e50 | Epoch 06 | 0m 4s | Train Loss 0.5674 Train Acc 70.24% | Val Loss 0.6418 Val Acc 65.04%\n",
      "rnn-Adam-e50 | Epoch 07 | 0m 4s | Train Loss 0.5137 Train Acc 74.83% | Val Loss 0.6202 Val Acc 67.90%\n",
      "rnn-Adam-e50 | Epoch 08 | 0m 4s | Train Loss 0.4671 Train Acc 77.98% | Val Loss 0.6061 Val Acc 69.55%\n",
      "rnn-Adam-e50 | Epoch 09 | 0m 4s | Train Loss 0.4366 Train Acc 80.00% | Val Loss 0.6239 Val Acc 70.52%\n",
      "rnn-Adam-e50 | Epoch 10 | 0m 4s | Train Loss 0.4068 Train Acc 81.72% | Val Loss 0.7353 Val Acc 57.03%\n",
      "rnn-Adam-e50 | Epoch 11 | 0m 4s | Train Loss 0.4398 Train Acc 79.33% | Val Loss 0.6925 Val Acc 59.89%\n",
      "rnn-Adam-e50 | Epoch 12 | 0m 4s | Train Loss 0.4401 Train Acc 79.13% | Val Loss 0.7201 Val Acc 61.16%\n",
      "rnn-Adam-e50 | Epoch 13 | 0m 4s | Train Loss 0.4150 Train Acc 80.50% | Val Loss 0.6898 Val Acc 64.90%\n",
      "rnn-Adam-e50 | Epoch 14 | 0m 4s | Train Loss 0.5730 Train Acc 69.09% | Val Loss 0.6853 Val Acc 61.99%\n",
      "rnn-Adam-e50 | Epoch 15 | 0m 4s | Train Loss 0.4469 Train Acc 78.46% | Val Loss 0.7203 Val Acc 63.19%\n",
      "rnn-Adam-e50 | Epoch 16 | 0m 4s | Train Loss 0.3639 Train Acc 83.97% | Val Loss 0.6885 Val Acc 68.59%\n",
      "rnn-Adam-e50 | Epoch 17 | 0m 4s | Train Loss 0.6302 Train Acc 64.66% | Val Loss 0.6702 Val Acc 55.71%\n",
      "rnn-Adam-e50 | Epoch 18 | 0m 4s | Train Loss 0.6307 Train Acc 65.08% | Val Loss 0.6651 Val Acc 58.06%\n",
      "rnn-Adam-e50 | Epoch 19 | 0m 4s | Train Loss 0.5942 Train Acc 68.06% | Val Loss 0.6800 Val Acc 58.56%\n",
      "rnn-Adam-e50 | Epoch 20 | 0m 4s | Train Loss 0.6010 Train Acc 66.31% | Val Loss 0.6690 Val Acc 58.49%\n",
      "rnn-Adam-e50 | Epoch 21 | 0m 4s | Train Loss 0.5712 Train Acc 69.44% | Val Loss 0.6830 Val Acc 59.29%\n",
      "rnn-Adam-e50 | Epoch 22 | 0m 4s | Train Loss 0.5526 Train Acc 70.81% | Val Loss 0.7002 Val Acc 59.66%\n",
      "rnn-Adam-e50 | Epoch 23 | 0m 4s | Train Loss 0.5230 Train Acc 72.63% | Val Loss 0.7125 Val Acc 59.68%\n",
      "rnn-Adam-e50 | Epoch 24 | 0m 4s | Train Loss 0.5029 Train Acc 74.34% | Val Loss 0.7116 Val Acc 59.92%\n",
      "rnn-Adam-e50 | Epoch 25 | 0m 4s | Train Loss 0.4943 Train Acc 74.88% | Val Loss 0.7285 Val Acc 60.36%\n",
      "rnn-Adam-e50 | Epoch 26 | 0m 4s | Train Loss 0.4693 Train Acc 76.71% | Val Loss 0.7391 Val Acc 59.49%\n",
      "rnn-Adam-e50 | Epoch 27 | 0m 4s | Train Loss 0.4560 Train Acc 77.70% | Val Loss 0.7688 Val Acc 60.15%\n",
      "rnn-Adam-e50 | Epoch 28 | 0m 4s | Train Loss 0.4461 Train Acc 78.45% | Val Loss 0.7622 Val Acc 60.33%\n",
      "rnn-Adam-e50 | Epoch 29 | 0m 4s | Train Loss 0.4309 Train Acc 79.49% | Val Loss 0.7605 Val Acc 61.04%\n",
      "rnn-Adam-e50 | Epoch 30 | 0m 4s | Train Loss 0.4054 Train Acc 80.88% | Val Loss 0.7920 Val Acc 61.18%\n",
      "rnn-Adam-e50 | Epoch 31 | 0m 4s | Train Loss 0.3924 Train Acc 81.79% | Val Loss 0.7904 Val Acc 61.85%\n",
      "rnn-Adam-e50 | Epoch 32 | 0m 4s | Train Loss 0.3775 Train Acc 82.45% | Val Loss 0.7942 Val Acc 62.40%\n",
      "rnn-Adam-e50 | Epoch 33 | 0m 4s | Train Loss 0.3566 Train Acc 83.39% | Val Loss 0.8277 Val Acc 62.61%\n",
      "rnn-Adam-e50 | Epoch 34 | 0m 4s | Train Loss 0.3881 Train Acc 81.85% | Val Loss 0.8014 Val Acc 63.31%\n",
      "rnn-Adam-e50 | Epoch 35 | 0m 4s | Train Loss 0.3350 Train Acc 85.11% | Val Loss 0.8243 Val Acc 63.69%\n",
      "rnn-Adam-e50 | Epoch 36 | 0m 4s | Train Loss 0.3145 Train Acc 86.03% | Val Loss 0.8229 Val Acc 64.99%\n",
      "rnn-Adam-e50 | Epoch 37 | 0m 4s | Train Loss 0.3010 Train Acc 87.19% | Val Loss 0.8441 Val Acc 65.28%\n",
      "rnn-Adam-e50 | Epoch 38 | 0m 4s | Train Loss 0.2789 Train Acc 88.22% | Val Loss 0.8470 Val Acc 65.68%\n",
      "rnn-Adam-e50 | Epoch 39 | 0m 4s | Train Loss 0.2896 Train Acc 87.67% | Val Loss 0.8177 Val Acc 66.77%\n",
      "rnn-Adam-e50 | Epoch 40 | 0m 4s | Train Loss 0.2721 Train Acc 88.39% | Val Loss 0.8328 Val Acc 67.47%\n",
      "rnn-Adam-e50 | Epoch 41 | 0m 4s | Train Loss 0.2525 Train Acc 89.63% | Val Loss 0.8554 Val Acc 67.50%\n",
      "rnn-Adam-e50 | Epoch 42 | 0m 4s | Train Loss 0.2607 Train Acc 89.21% | Val Loss 0.8786 Val Acc 65.34%\n",
      "rnn-Adam-e50 | Epoch 43 | 0m 4s | Train Loss 0.3325 Train Acc 85.91% | Val Loss 0.8233 Val Acc 67.01%\n",
      "rnn-Adam-e50 | Epoch 44 | 0m 4s | Train Loss 0.2694 Train Acc 88.85% | Val Loss 0.8410 Val Acc 63.78%\n",
      "rnn-Adam-e50 | Epoch 45 | 0m 4s | Train Loss 0.2387 Train Acc 90.30% | Val Loss 0.8569 Val Acc 66.94%\n",
      "rnn-Adam-e50 | Epoch 46 | 0m 4s | Train Loss 0.2290 Train Acc 90.70% | Val Loss 0.8890 Val Acc 66.32%\n",
      "rnn-Adam-e50 | Epoch 47 | 0m 4s | Train Loss 0.2102 Train Acc 91.50% | Val Loss 0.9074 Val Acc 66.54%\n",
      "rnn-Adam-e50 | Epoch 48 | 0m 4s | Train Loss 0.2636 Train Acc 88.93% | Val Loss 0.8922 Val Acc 67.92%\n",
      "rnn-Adam-e50 | Epoch 49 | 0m 4s | Train Loss 0.2408 Train Acc 90.14% | Val Loss 0.9124 Val Acc 68.20%\n",
      "rnn-Adam-e50 | Epoch 50 | 0m 4s | Train Loss 0.2237 Train Acc 90.93% | Val Loss 0.8983 Val Acc 67.63%\n",
      "RNN optimizer/epoch sweeps complete. Per-epoch checkpoints and histories are in models/ and history/.\n"
     ]
    }
   ],
   "source": [
    "def rnn_optimizer_and_epoch_sweeps(TEXT, train_iter, valid_iter, output_prefix=\"results\"):\n",
    "    base = build_models(len(TEXT.vocab))['rnn']\n",
    "\n",
    "    # (2) Optimizer Sweep\n",
    "    optimizers_to_try = {\n",
    "        \"SGD\": (optim.SGD, {\"lr\": 1e-3}), # (1) Sample code reimplementation\n",
    "        #\"Adam\": (optim.Adam, {\"lr\": 1e-3}), Adam is tested in next loop\n",
    "        \"Adagrad\": (optim.Adagrad, {\"lr\": 1e-3})\n",
    "    }\n",
    "\n",
    "    # sweep optimizers with 50 epochs\n",
    "    for opt_name, (opt_ctor, opt_kwargs) in optimizers_to_try.items():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        run_name = f\"rnn-{opt_name}\"\n",
    "        model = deepcopy(base)\n",
    "        run_and_record(run_name, model, train_iter, valid_iter, n_epochs=20, lr=opt_kwargs.get(\"lr\", 1e-3), output_prefix=output_prefix)\n",
    "\n",
    "    # (3) Adam epoch sweep (5,10,20,50)\n",
    "    for n_epochs in [5,10,20,50]:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        run_name = f\"rnn-Adam-e{n_epochs}\"\n",
    "        model = deepcopy(base)\n",
    "        run_and_record(run_name, model, train_iter, valid_iter, n_epochs=n_epochs, lr=1e-3, output_prefix=output_prefix)\n",
    "\n",
    "    print(\"RNN optimizer/epoch sweeps complete. Per-epoch checkpoints and histories are in models/ and history/.\")\n",
    "\n",
    "rnn_optimizer_and_epoch_sweeps(TEXT, train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8b2f7",
   "metadata": {},
   "source": [
    "Use Adam optimizer, 50 epochs, and randomly initialized embeddings, run the \n",
    "experiments with the following models: \n",
    "1. One-layer feed forward neural network, hidden dimension is 500.  \n",
    "    model: ff1\n",
    "2. Two-layer feed forward neural network, hidden dimensions are 500 and 300.  \n",
    "    model: ff2\n",
    "3. Three-layer feed-forward neural network, hidden dimensions are 500, 300 and 200  \n",
    "    model: ff3\n",
    "4. CNN model (using three feature maps with the sizes of feature map are 1, 2, and 3)  \n",
    "    model:cnn\n",
    "5. LSTM model  \n",
    "    model: lstm\n",
    "6. Bi-LSTM model  \n",
    "    model: bilstm\n",
    "\n",
    "These are called in `build_models` method:  \n",
    "    models['ff1'] = FeedForwardText(INPUT_DIM, EMBEDDING_DIM, hidden_dims=[500], output_dim=OUTPUT_DIM)  \n",
    "    models['ff2'] = FeedForwardText(INPUT_DIM, EMBEDDING_DIM, hidden_dims=[500,300], output_dim=OUTPUT_DIM)  \n",
    "    models['ff3'] = FeedForwardText(INPUT_DIM, EMBEDDING_DIM, hidden_dims=[500,300,200], output_dim=OUTPUT_DIM)  \n",
    "    models['cnn'] = CNNText(INPUT_DIM, EMBEDDING_DIM, n_filters=100, filter_sizes=(1,2,3), output_dim=OUTPUT_DIM, padding_idx=None)  \n",
    "    models['lstm'] = LSTMModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, n_layers=1, dropout=0.5)  \n",
    "    models['bilstm'] = BiLSTMModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, n_layers=1, dropout=0.5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d3ac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting ff1 (Adam, 50 epochs)\n",
      "ff1 | Epoch 01 | 0m 2s | Train Loss 0.5885 Train Acc 67.72% | Val Loss 0.4525 Val Acc 79.14%\n",
      "ff1 | Epoch 02 | 0m 2s | Train Loss 0.3651 Train Acc 84.36% | Val Loss 0.3412 Val Acc 85.94%\n",
      "ff1 | Epoch 03 | 0m 2s | Train Loss 0.2735 Train Acc 89.05% | Val Loss 0.3295 Val Acc 86.39%\n",
      "ff1 | Epoch 04 | 0m 2s | Train Loss 0.2253 Train Acc 91.81% | Val Loss 0.2995 Val Acc 88.06%\n",
      "ff1 | Epoch 05 | 0m 2s | Train Loss 0.1751 Train Acc 93.88% | Val Loss 0.3136 Val Acc 87.82%\n",
      "ff1 | Epoch 06 | 0m 2s | Train Loss 0.1255 Train Acc 95.77% | Val Loss 0.3167 Val Acc 88.40%\n",
      "ff1 | Epoch 07 | 0m 2s | Train Loss 0.1004 Train Acc 96.81% | Val Loss 0.4011 Val Acc 87.55%\n",
      "ff1 | Epoch 08 | 0m 2s | Train Loss 0.1178 Train Acc 97.43% | Val Loss 0.3657 Val Acc 88.43%\n",
      "ff1 | Epoch 09 | 0m 2s | Train Loss 0.0922 Train Acc 98.07% | Val Loss 0.3909 Val Acc 88.23%\n",
      "ff1 | Epoch 10 | 0m 2s | Train Loss 0.0373 Train Acc 99.13% | Val Loss 0.4263 Val Acc 87.96%\n",
      "ff1 | Epoch 11 | 0m 2s | Train Loss 0.0515 Train Acc 98.98% | Val Loss 0.4703 Val Acc 87.84%\n",
      "ff1 | Epoch 12 | 0m 2s | Train Loss 0.0339 Train Acc 99.26% | Val Loss 0.4722 Val Acc 88.07%\n",
      "ff1 | Epoch 13 | 0m 2s | Train Loss 0.0327 Train Acc 99.29% | Val Loss 0.5173 Val Acc 87.96%\n",
      "ff1 | Epoch 14 | 0m 2s | Train Loss 0.0117 Train Acc 99.76% | Val Loss 0.5334 Val Acc 88.09%\n",
      "ff1 | Epoch 15 | 0m 2s | Train Loss 0.0288 Train Acc 99.44% | Val Loss 0.6217 Val Acc 87.21%\n",
      "ff1 | Epoch 16 | 0m 2s | Train Loss 0.0814 Train Acc 99.21% | Val Loss 0.5685 Val Acc 87.93%\n",
      "ff1 | Epoch 17 | 0m 2s | Train Loss 0.0234 Train Acc 99.51% | Val Loss 0.5870 Val Acc 88.06%\n",
      "ff1 | Epoch 18 | 0m 2s | Train Loss 0.0210 Train Acc 99.59% | Val Loss 0.6556 Val Acc 87.43%\n",
      "ff1 | Epoch 19 | 0m 2s | Train Loss 0.0540 Train Acc 99.38% | Val Loss 0.6208 Val Acc 87.94%\n",
      "ff1 | Epoch 20 | 0m 2s | Train Loss 0.0141 Train Acc 99.70% | Val Loss 0.6434 Val Acc 87.85%\n",
      "ff1 | Epoch 21 | 0m 2s | Train Loss 0.0036 Train Acc 99.94% | Val Loss 0.6601 Val Acc 87.73%\n",
      "ff1 | Epoch 22 | 0m 2s | Train Loss 0.0054 Train Acc 99.83% | Val Loss 0.6759 Val Acc 87.73%\n",
      "ff1 | Epoch 23 | 0m 2s | Train Loss 0.0411 Train Acc 99.56% | Val Loss 0.6741 Val Acc 87.60%\n",
      "ff1 | Epoch 24 | 0m 2s | Train Loss 0.0056 Train Acc 99.84% | Val Loss 0.6990 Val Acc 87.61%\n",
      "ff1 | Epoch 25 | 0m 2s | Train Loss 0.0032 Train Acc 99.92% | Val Loss 0.7302 Val Acc 87.37%\n",
      "ff1 | Epoch 26 | 0m 2s | Train Loss 0.0020 Train Acc 99.97% | Val Loss 0.7224 Val Acc 87.73%\n",
      "ff1 | Epoch 27 | 0m 2s | Train Loss 0.0017 Train Acc 99.97% | Val Loss 0.7350 Val Acc 87.78%\n",
      "ff1 | Epoch 28 | 0m 2s | Train Loss 0.0015 Train Acc 99.97% | Val Loss 0.7816 Val Acc 87.46%\n",
      "ff1 | Epoch 29 | 0m 2s | Train Loss 0.0596 Train Acc 99.48% | Val Loss 0.7834 Val Acc 87.43%\n",
      "ff1 | Epoch 30 | 0m 2s | Train Loss 0.0014 Train Acc 99.97% | Val Loss 0.7715 Val Acc 87.80%\n",
      "ff1 | Epoch 31 | 0m 2s | Train Loss 0.0012 Train Acc 99.97% | Val Loss 0.7845 Val Acc 87.66%\n",
      "ff1 | Epoch 32 | 0m 2s | Train Loss 0.0016 Train Acc 99.95% | Val Loss 0.8041 Val Acc 87.57%\n",
      "ff1 | Epoch 33 | 0m 2s | Train Loss 0.0036 Train Acc 99.91% | Val Loss 0.8162 Val Acc 87.57%\n",
      "ff1 | Epoch 34 | 0m 2s | Train Loss 0.0007 Train Acc 99.99% | Val Loss 0.8295 Val Acc 87.53%\n",
      "ff1 | Epoch 35 | 0m 2s | Train Loss 0.0008 Train Acc 99.99% | Val Loss 0.8458 Val Acc 87.67%\n",
      "ff1 | Epoch 36 | 0m 2s | Train Loss 0.0004 Train Acc 100.00% | Val Loss 0.8582 Val Acc 87.61%\n",
      "ff1 | Epoch 37 | 0m 2s | Train Loss 0.0004 Train Acc 99.99% | Val Loss 0.8690 Val Acc 87.57%\n",
      "ff1 | Epoch 38 | 0m 2s | Train Loss 0.0005 Train Acc 99.99% | Val Loss 0.9168 Val Acc 87.28%\n",
      "ff1 | Epoch 39 | 0m 2s | Train Loss 0.0175 Train Acc 99.61% | Val Loss 1.1665 Val Acc 86.18%\n",
      "ff1 | Epoch 40 | 0m 2s | Train Loss 0.0635 Train Acc 99.30% | Val Loss 0.9797 Val Acc 86.87%\n",
      "ff1 | Epoch 41 | 0m 2s | Train Loss 0.0103 Train Acc 99.77% | Val Loss 0.9861 Val Acc 87.36%\n",
      "ff1 | Epoch 42 | 0m 2s | Train Loss 0.0475 Train Acc 99.50% | Val Loss 0.9907 Val Acc 87.25%\n",
      "ff1 | Epoch 43 | 0m 2s | Train Loss 0.0006 Train Acc 99.98% | Val Loss 0.9764 Val Acc 87.62%\n",
      "ff1 | Epoch 44 | 0m 2s | Train Loss 0.0005 Train Acc 99.99% | Val Loss 0.9825 Val Acc 87.53%\n",
      "ff1 | Epoch 45 | 0m 2s | Train Loss 0.0004 Train Acc 99.98% | Val Loss 0.9928 Val Acc 87.53%\n",
      "ff1 | Epoch 46 | 0m 2s | Train Loss 0.0016 Train Acc 99.95% | Val Loss 1.0083 Val Acc 87.53%\n",
      "ff1 | Epoch 47 | 0m 2s | Train Loss 0.0020 Train Acc 99.93% | Val Loss 1.0160 Val Acc 87.31%\n",
      "ff1 | Epoch 48 | 0m 2s | Train Loss 0.0005 Train Acc 99.99% | Val Loss 1.0248 Val Acc 87.46%\n",
      "ff1 | Epoch 49 | 0m 2s | Train Loss 0.0005 Train Acc 99.98% | Val Loss 1.0395 Val Acc 87.65%\n",
      "ff1 | Epoch 50 | 0m 2s | Train Loss 0.0187 Train Acc 99.78% | Val Loss 1.0527 Val Acc 87.30%\n",
      "\n",
      "================================================================================\n",
      "Starting ff2 (Adam, 50 epochs)\n",
      "ff2 | Epoch 01 | 0m 2s | Train Loss 0.5546 Train Acc 71.07% | Val Loss 0.4150 Val Acc 81.18%\n",
      "ff2 | Epoch 02 | 0m 2s | Train Loss 0.3633 Train Acc 84.23% | Val Loss 0.3439 Val Acc 84.98%\n",
      "ff2 | Epoch 03 | 0m 2s | Train Loss 0.2633 Train Acc 89.43% | Val Loss 0.3504 Val Acc 85.35%\n",
      "ff2 | Epoch 04 | 0m 2s | Train Loss 0.2041 Train Acc 91.84% | Val Loss 0.3202 Val Acc 87.72%\n",
      "ff2 | Epoch 05 | 0m 2s | Train Loss 0.1541 Train Acc 94.45% | Val Loss 0.3362 Val Acc 87.40%\n",
      "ff2 | Epoch 06 | 0m 2s | Train Loss 0.1123 Train Acc 96.14% | Val Loss 0.3791 Val Acc 88.14%\n",
      "ff2 | Epoch 07 | 0m 2s | Train Loss 0.0788 Train Acc 97.21% | Val Loss 0.4252 Val Acc 87.70%\n",
      "ff2 | Epoch 08 | 0m 2s | Train Loss 0.0571 Train Acc 97.92% | Val Loss 0.5257 Val Acc 87.70%\n",
      "ff2 | Epoch 09 | 0m 2s | Train Loss 0.0410 Train Acc 98.65% | Val Loss 0.5684 Val Acc 87.54%\n",
      "ff2 | Epoch 10 | 0m 2s | Train Loss 0.0335 Train Acc 98.88% | Val Loss 0.6671 Val Acc 87.52%\n",
      "ff2 | Epoch 11 | 0m 2s | Train Loss 0.0391 Train Acc 98.57% | Val Loss 0.6951 Val Acc 87.01%\n",
      "ff2 | Epoch 12 | 0m 2s | Train Loss 0.0207 Train Acc 99.25% | Val Loss 0.8210 Val Acc 87.54%\n",
      "ff2 | Epoch 13 | 0m 2s | Train Loss 0.0293 Train Acc 98.98% | Val Loss 0.7714 Val Acc 87.23%\n",
      "ff2 | Epoch 14 | 0m 2s | Train Loss 0.0166 Train Acc 99.23% | Val Loss 0.9074 Val Acc 87.46%\n",
      "ff2 | Epoch 15 | 0m 2s | Train Loss 0.0109 Train Acc 99.60% | Val Loss 0.9872 Val Acc 87.31%\n",
      "ff2 | Epoch 16 | 0m 2s | Train Loss 0.0146 Train Acc 99.32% | Val Loss 0.9508 Val Acc 87.03%\n",
      "ff2 | Epoch 17 | 0m 2s | Train Loss 0.0120 Train Acc 99.50% | Val Loss 1.0935 Val Acc 86.59%\n",
      "ff2 | Epoch 18 | 0m 2s | Train Loss 0.0326 Train Acc 99.11% | Val Loss 1.0958 Val Acc 86.92%\n",
      "ff2 | Epoch 19 | 0m 2s | Train Loss 0.0160 Train Acc 99.37% | Val Loss 1.1016 Val Acc 87.30%\n",
      "ff2 | Epoch 20 | 0m 2s | Train Loss 0.0200 Train Acc 99.35% | Val Loss 1.0596 Val Acc 87.44%\n",
      "ff2 | Epoch 21 | 0m 2s | Train Loss 0.0085 Train Acc 99.64% | Val Loss 1.1952 Val Acc 87.35%\n",
      "ff2 | Epoch 22 | 0m 2s | Train Loss 0.0058 Train Acc 99.78% | Val Loss 1.3516 Val Acc 87.35%\n",
      "ff2 | Epoch 23 | 0m 2s | Train Loss 0.0518 Train Acc 99.34% | Val Loss 1.0302 Val Acc 87.27%\n",
      "ff2 | Epoch 24 | 0m 2s | Train Loss 0.0082 Train Acc 99.57% | Val Loss 1.1760 Val Acc 86.78%\n",
      "ff2 | Epoch 25 | 0m 2s | Train Loss 0.0119 Train Acc 99.55% | Val Loss 1.1937 Val Acc 87.29%\n",
      "ff2 | Epoch 26 | 0m 2s | Train Loss 0.0172 Train Acc 99.45% | Val Loss 1.2853 Val Acc 87.17%\n",
      "ff2 | Epoch 27 | 0m 2s | Train Loss 0.0084 Train Acc 99.63% | Val Loss 1.3093 Val Acc 87.28%\n",
      "ff2 | Epoch 28 | 0m 2s | Train Loss 0.0120 Train Acc 99.51% | Val Loss 1.1833 Val Acc 87.08%\n",
      "ff2 | Epoch 29 | 0m 2s | Train Loss 0.0090 Train Acc 99.69% | Val Loss 1.2882 Val Acc 87.37%\n",
      "ff2 | Epoch 30 | 0m 2s | Train Loss 0.0142 Train Acc 99.64% | Val Loss 1.3709 Val Acc 85.77%\n",
      "ff2 | Epoch 31 | 0m 2s | Train Loss 0.0121 Train Acc 99.54% | Val Loss 1.3101 Val Acc 87.13%\n",
      "ff2 | Epoch 32 | 0m 2s | Train Loss 0.0091 Train Acc 99.59% | Val Loss 1.3759 Val Acc 87.45%\n",
      "ff2 | Epoch 33 | 0m 2s | Train Loss 0.0053 Train Acc 99.82% | Val Loss 1.5267 Val Acc 86.67%\n",
      "ff2 | Epoch 34 | 0m 2s | Train Loss 0.0274 Train Acc 99.13% | Val Loss 1.1524 Val Acc 87.07%\n",
      "ff2 | Epoch 35 | 0m 2s | Train Loss 0.0057 Train Acc 99.75% | Val Loss 1.3787 Val Acc 87.45%\n",
      "ff2 | Epoch 36 | 0m 2s | Train Loss 0.0093 Train Acc 99.61% | Val Loss 1.4232 Val Acc 87.44%\n",
      "ff2 | Epoch 37 | 0m 2s | Train Loss 0.0134 Train Acc 99.72% | Val Loss 1.3876 Val Acc 87.22%\n",
      "ff2 | Epoch 38 | 0m 2s | Train Loss 0.0063 Train Acc 99.69% | Val Loss 1.5466 Val Acc 87.28%\n",
      "ff2 | Epoch 39 | 0m 2s | Train Loss 0.0400 Train Acc 99.40% | Val Loss 1.1653 Val Acc 86.90%\n",
      "ff2 | Epoch 40 | 0m 2s | Train Loss 0.0095 Train Acc 99.56% | Val Loss 1.3889 Val Acc 87.12%\n",
      "ff2 | Epoch 41 | 0m 2s | Train Loss 0.0069 Train Acc 99.75% | Val Loss 1.4856 Val Acc 87.31%\n",
      "ff2 | Epoch 42 | 0m 2s | Train Loss 0.0069 Train Acc 99.73% | Val Loss 1.6529 Val Acc 87.26%\n",
      "ff2 | Epoch 43 | 0m 2s | Train Loss 0.0054 Train Acc 99.80% | Val Loss 1.6718 Val Acc 87.32%\n",
      "ff2 | Epoch 44 | 0m 2s | Train Loss 0.0130 Train Acc 99.54% | Val Loss 1.3383 Val Acc 86.85%\n",
      "ff2 | Epoch 45 | 0m 2s | Train Loss 0.0201 Train Acc 99.30% | Val Loss 1.3805 Val Acc 87.25%\n",
      "ff2 | Epoch 46 | 0m 2s | Train Loss 0.0051 Train Acc 99.77% | Val Loss 1.5008 Val Acc 87.20%\n",
      "ff2 | Epoch 47 | 0m 2s | Train Loss 0.0138 Train Acc 99.63% | Val Loss 1.6364 Val Acc 87.17%\n",
      "ff2 | Epoch 48 | 0m 2s | Train Loss 0.0051 Train Acc 99.81% | Val Loss 1.7620 Val Acc 87.03%\n",
      "ff2 | Epoch 49 | 0m 2s | Train Loss 0.0291 Train Acc 99.28% | Val Loss 1.4624 Val Acc 87.19%\n",
      "ff2 | Epoch 50 | 0m 2s | Train Loss 0.0079 Train Acc 99.63% | Val Loss 1.5757 Val Acc 87.37%\n",
      "\n",
      "================================================================================\n",
      "Starting ff3 (Adam, 50 epochs)\n",
      "ff3 | Epoch 01 | 0m 2s | Train Loss 0.5876 Train Acc 67.88% | Val Loss 0.4441 Val Acc 80.16%\n",
      "ff3 | Epoch 02 | 0m 2s | Train Loss 0.3876 Train Acc 83.26% | Val Loss 0.3583 Val Acc 85.09%\n",
      "ff3 | Epoch 03 | 0m 2s | Train Loss 0.2919 Train Acc 88.27% | Val Loss 0.3301 Val Acc 86.18%\n",
      "ff3 | Epoch 04 | 0m 2s | Train Loss 0.2221 Train Acc 91.56% | Val Loss 0.3208 Val Acc 87.84%\n",
      "ff3 | Epoch 05 | 0m 2s | Train Loss 0.1668 Train Acc 93.90% | Val Loss 0.3659 Val Acc 87.64%\n",
      "ff3 | Epoch 06 | 0m 2s | Train Loss 0.1260 Train Acc 95.45% | Val Loss 0.3907 Val Acc 87.24%\n",
      "ff3 | Epoch 07 | 0m 2s | Train Loss 0.1254 Train Acc 96.50% | Val Loss 0.4258 Val Acc 87.56%\n",
      "ff3 | Epoch 08 | 0m 2s | Train Loss 0.0722 Train Acc 97.65% | Val Loss 0.4716 Val Acc 87.44%\n",
      "ff3 | Epoch 09 | 0m 2s | Train Loss 0.0491 Train Acc 98.35% | Val Loss 0.5644 Val Acc 87.00%\n",
      "ff3 | Epoch 10 | 0m 2s | Train Loss 0.0418 Train Acc 98.39% | Val Loss 0.6543 Val Acc 87.40%\n",
      "ff3 | Epoch 11 | 0m 2s | Train Loss 0.0291 Train Acc 98.96% | Val Loss 0.7478 Val Acc 87.31%\n",
      "ff3 | Epoch 12 | 0m 2s | Train Loss 0.0276 Train Acc 98.99% | Val Loss 0.7149 Val Acc 87.05%\n",
      "ff3 | Epoch 13 | 0m 2s | Train Loss 0.0239 Train Acc 99.19% | Val Loss 0.7194 Val Acc 86.58%\n",
      "ff3 | Epoch 14 | 0m 2s | Train Loss 0.0171 Train Acc 99.29% | Val Loss 1.0143 Val Acc 87.25%\n",
      "ff3 | Epoch 15 | 0m 2s | Train Loss 0.0255 Train Acc 99.04% | Val Loss 0.9395 Val Acc 87.07%\n",
      "ff3 | Epoch 16 | 0m 2s | Train Loss 0.0177 Train Acc 99.44% | Val Loss 0.9369 Val Acc 87.53%\n",
      "ff3 | Epoch 17 | 0m 2s | Train Loss 0.0149 Train Acc 99.30% | Val Loss 0.9676 Val Acc 87.23%\n",
      "ff3 | Epoch 18 | 0m 2s | Train Loss 0.0155 Train Acc 99.35% | Val Loss 1.0428 Val Acc 85.70%\n",
      "ff3 | Epoch 19 | 0m 2s | Train Loss 0.0112 Train Acc 99.48% | Val Loss 1.1981 Val Acc 86.31%\n",
      "ff3 | Epoch 20 | 0m 2s | Train Loss 0.0120 Train Acc 99.50% | Val Loss 1.3042 Val Acc 87.29%\n",
      "ff3 | Epoch 21 | 0m 2s | Train Loss 0.0303 Train Acc 98.81% | Val Loss 0.8404 Val Acc 86.72%\n",
      "ff3 | Epoch 22 | 0m 2s | Train Loss 0.0156 Train Acc 99.38% | Val Loss 1.0444 Val Acc 87.14%\n",
      "ff3 | Epoch 23 | 0m 2s | Train Loss 0.0126 Train Acc 99.33% | Val Loss 1.2503 Val Acc 86.66%\n",
      "ff3 | Epoch 24 | 0m 2s | Train Loss 0.0112 Train Acc 99.44% | Val Loss 1.3344 Val Acc 87.42%\n",
      "ff3 | Epoch 25 | 0m 2s | Train Loss 0.0187 Train Acc 99.26% | Val Loss 1.2137 Val Acc 87.31%\n",
      "ff3 | Epoch 26 | 0m 2s | Train Loss 0.0112 Train Acc 99.37% | Val Loss 1.3721 Val Acc 86.96%\n",
      "ff3 | Epoch 27 | 0m 2s | Train Loss 0.0087 Train Acc 99.57% | Val Loss 1.3586 Val Acc 87.27%\n",
      "ff3 | Epoch 28 | 0m 2s | Train Loss 0.0123 Train Acc 99.44% | Val Loss 1.2940 Val Acc 87.29%\n",
      "ff3 | Epoch 29 | 0m 2s | Train Loss 0.0143 Train Acc 99.66% | Val Loss 1.0851 Val Acc 87.08%\n",
      "ff3 | Epoch 30 | 0m 2s | Train Loss 0.0103 Train Acc 99.40% | Val Loss 1.5305 Val Acc 87.07%\n",
      "ff3 | Epoch 31 | 0m 2s | Train Loss 0.0106 Train Acc 99.63% | Val Loss 1.3802 Val Acc 87.20%\n",
      "ff3 | Epoch 32 | 0m 2s | Train Loss 0.0096 Train Acc 99.65% | Val Loss 1.5198 Val Acc 87.06%\n",
      "ff3 | Epoch 33 | 0m 2s | Train Loss 0.0163 Train Acc 99.44% | Val Loss 1.2914 Val Acc 87.05%\n",
      "ff3 | Epoch 34 | 0m 2s | Train Loss 0.0100 Train Acc 99.53% | Val Loss 1.3897 Val Acc 87.17%\n",
      "ff3 | Epoch 35 | 0m 2s | Train Loss 0.0248 Train Acc 99.36% | Val Loss 0.9513 Val Acc 86.48%\n",
      "ff3 | Epoch 36 | 0m 2s | Train Loss 0.0091 Train Acc 99.55% | Val Loss 1.5163 Val Acc 87.01%\n",
      "ff3 | Epoch 37 | 0m 2s | Train Loss 0.0140 Train Acc 99.31% | Val Loss 1.6665 Val Acc 87.01%\n",
      "ff3 | Epoch 38 | 0m 2s | Train Loss 0.0068 Train Acc 99.74% | Val Loss 1.6131 Val Acc 87.00%\n",
      "ff3 | Epoch 39 | 0m 2s | Train Loss 0.0286 Train Acc 99.02% | Val Loss 1.2635 Val Acc 86.89%\n",
      "ff3 | Epoch 40 | 0m 2s | Train Loss 0.0079 Train Acc 99.53% | Val Loss 1.9653 Val Acc 87.21%\n",
      "ff3 | Epoch 41 | 0m 2s | Train Loss 0.0111 Train Acc 99.51% | Val Loss 1.5286 Val Acc 87.16%\n",
      "ff3 | Epoch 42 | 0m 2s | Train Loss 0.0516 Train Acc 99.35% | Val Loss 1.2645 Val Acc 86.75%\n",
      "ff3 | Epoch 43 | 0m 2s | Train Loss 0.0112 Train Acc 99.45% | Val Loss 1.9108 Val Acc 86.88%\n",
      "ff3 | Epoch 44 | 0m 2s | Train Loss 0.0101 Train Acc 99.56% | Val Loss 1.6599 Val Acc 87.12%\n",
      "ff3 | Epoch 45 | 0m 2s | Train Loss 0.0053 Train Acc 99.70% | Val Loss 2.1420 Val Acc 87.39%\n",
      "ff3 | Epoch 46 | 0m 2s | Train Loss 0.0112 Train Acc 99.58% | Val Loss 1.7897 Val Acc 87.16%\n",
      "ff3 | Epoch 47 | 0m 2s | Train Loss 0.0053 Train Acc 99.76% | Val Loss 1.7474 Val Acc 87.04%\n",
      "ff3 | Epoch 48 | 0m 2s | Train Loss 0.0124 Train Acc 99.60% | Val Loss 1.9373 Val Acc 87.31%\n",
      "ff3 | Epoch 49 | 0m 2s | Train Loss 0.0143 Train Acc 99.51% | Val Loss 2.1464 Val Acc 86.56%\n",
      "ff3 | Epoch 50 | 0m 2s | Train Loss 0.0084 Train Acc 99.55% | Val Loss 1.9535 Val Acc 87.25%\n",
      "\n",
      "================================================================================\n",
      "Starting cnn (Adam, 50 epochs)\n",
      "cnn | Epoch 01 | 0m 5s | Train Loss 0.6521 Train Acc 63.34% | Val Loss 0.4775 Val Acc 79.85%\n",
      "cnn | Epoch 02 | 0m 4s | Train Loss 0.4963 Train Acc 76.11% | Val Loss 0.4033 Val Acc 83.40%\n",
      "cnn | Epoch 03 | 0m 4s | Train Loss 0.4413 Train Acc 79.21% | Val Loss 0.3637 Val Acc 84.99%\n",
      "cnn | Epoch 04 | 0m 4s | Train Loss 0.4007 Train Acc 81.89% | Val Loss 0.3393 Val Acc 86.07%\n",
      "cnn | Epoch 05 | 0m 4s | Train Loss 0.3679 Train Acc 84.01% | Val Loss 0.3321 Val Acc 85.61%\n",
      "cnn | Epoch 06 | 0m 4s | Train Loss 0.3325 Train Acc 85.76% | Val Loss 0.3071 Val Acc 86.95%\n",
      "cnn | Epoch 07 | 0m 4s | Train Loss 0.3046 Train Acc 86.99% | Val Loss 0.2936 Val Acc 87.72%\n",
      "cnn | Epoch 08 | 0m 4s | Train Loss 0.2770 Train Acc 88.58% | Val Loss 0.2929 Val Acc 87.64%\n",
      "cnn | Epoch 09 | 0m 4s | Train Loss 0.2474 Train Acc 90.03% | Val Loss 0.2850 Val Acc 88.29%\n",
      "cnn | Epoch 10 | 0m 4s | Train Loss 0.2214 Train Acc 91.22% | Val Loss 0.2806 Val Acc 88.31%\n",
      "cnn | Epoch 11 | 0m 4s | Train Loss 0.1924 Train Acc 92.38% | Val Loss 0.2845 Val Acc 88.50%\n",
      "cnn | Epoch 12 | 0m 4s | Train Loss 0.1700 Train Acc 93.27% | Val Loss 0.3050 Val Acc 87.42%\n",
      "cnn | Epoch 13 | 0m 4s | Train Loss 0.1414 Train Acc 94.63% | Val Loss 0.3016 Val Acc 88.19%\n",
      "cnn | Epoch 14 | 0m 4s | Train Loss 0.1219 Train Acc 95.27% | Val Loss 0.3141 Val Acc 88.10%\n",
      "cnn | Epoch 15 | 0m 4s | Train Loss 0.0985 Train Acc 96.23% | Val Loss 0.3415 Val Acc 87.65%\n",
      "cnn | Epoch 16 | 0m 4s | Train Loss 0.0853 Train Acc 96.89% | Val Loss 0.3673 Val Acc 87.33%\n",
      "cnn | Epoch 17 | 0m 4s | Train Loss 0.0693 Train Acc 97.60% | Val Loss 0.3745 Val Acc 87.50%\n",
      "cnn | Epoch 18 | 0m 4s | Train Loss 0.0596 Train Acc 97.97% | Val Loss 0.3957 Val Acc 87.41%\n",
      "cnn | Epoch 19 | 0m 4s | Train Loss 0.0458 Train Acc 98.38% | Val Loss 0.4231 Val Acc 87.50%\n",
      "cnn | Epoch 20 | 0m 4s | Train Loss 0.0420 Train Acc 98.62% | Val Loss 0.4551 Val Acc 87.13%\n",
      "cnn | Epoch 21 | 0m 4s | Train Loss 0.0345 Train Acc 98.87% | Val Loss 0.4773 Val Acc 87.36%\n",
      "cnn | Epoch 22 | 0m 4s | Train Loss 0.0314 Train Acc 98.89% | Val Loss 0.5072 Val Acc 86.99%\n",
      "cnn | Epoch 23 | 0m 4s | Train Loss 0.0298 Train Acc 99.05% | Val Loss 0.5365 Val Acc 86.62%\n",
      "cnn | Epoch 24 | 0m 4s | Train Loss 0.0241 Train Acc 99.22% | Val Loss 0.5560 Val Acc 86.48%\n",
      "cnn | Epoch 25 | 0m 3s | Train Loss 0.0205 Train Acc 99.34% | Val Loss 0.5710 Val Acc 86.60%\n",
      "cnn | Epoch 26 | 0m 4s | Train Loss 0.0170 Train Acc 99.49% | Val Loss 0.6074 Val Acc 86.37%\n",
      "cnn | Epoch 27 | 0m 4s | Train Loss 0.0129 Train Acc 99.60% | Val Loss 0.6563 Val Acc 86.05%\n",
      "cnn | Epoch 28 | 0m 4s | Train Loss 0.0140 Train Acc 99.53% | Val Loss 0.6782 Val Acc 86.20%\n",
      "cnn | Epoch 29 | 0m 4s | Train Loss 0.0141 Train Acc 99.47% | Val Loss 0.7040 Val Acc 85.84%\n",
      "cnn | Epoch 30 | 0m 4s | Train Loss 0.0110 Train Acc 99.65% | Val Loss 0.7585 Val Acc 85.61%\n",
      "cnn | Epoch 31 | 0m 4s | Train Loss 0.0130 Train Acc 99.62% | Val Loss 0.7617 Val Acc 85.70%\n",
      "cnn | Epoch 32 | 0m 4s | Train Loss 0.0105 Train Acc 99.62% | Val Loss 0.7933 Val Acc 85.72%\n",
      "cnn | Epoch 33 | 0m 4s | Train Loss 0.0131 Train Acc 99.58% | Val Loss 0.8103 Val Acc 85.75%\n",
      "cnn | Epoch 34 | 0m 4s | Train Loss 0.0099 Train Acc 99.70% | Val Loss 0.8643 Val Acc 85.13%\n",
      "cnn | Epoch 35 | 0m 4s | Train Loss 0.0087 Train Acc 99.72% | Val Loss 0.8694 Val Acc 85.64%\n",
      "cnn | Epoch 36 | 0m 4s | Train Loss 0.0109 Train Acc 99.64% | Val Loss 0.9065 Val Acc 85.60%\n",
      "cnn | Epoch 37 | 0m 4s | Train Loss 0.0077 Train Acc 99.76% | Val Loss 0.9087 Val Acc 85.51%\n",
      "cnn | Epoch 38 | 0m 4s | Train Loss 0.0068 Train Acc 99.79% | Val Loss 0.9398 Val Acc 85.28%\n",
      "cnn | Epoch 39 | 0m 4s | Train Loss 0.0078 Train Acc 99.76% | Val Loss 0.9889 Val Acc 85.13%\n",
      "cnn | Epoch 40 | 0m 4s | Train Loss 0.0078 Train Acc 99.76% | Val Loss 0.9933 Val Acc 85.36%\n",
      "cnn | Epoch 41 | 0m 4s | Train Loss 0.0070 Train Acc 99.78% | Val Loss 1.0000 Val Acc 85.40%\n",
      "cnn | Epoch 42 | 0m 4s | Train Loss 0.0092 Train Acc 99.69% | Val Loss 1.0288 Val Acc 85.50%\n",
      "cnn | Epoch 43 | 0m 4s | Train Loss 0.0081 Train Acc 99.79% | Val Loss 1.0345 Val Acc 85.60%\n",
      "cnn | Epoch 44 | 0m 4s | Train Loss 0.0068 Train Acc 99.78% | Val Loss 1.0768 Val Acc 85.40%\n",
      "cnn | Epoch 45 | 0m 4s | Train Loss 0.0062 Train Acc 99.81% | Val Loss 1.1035 Val Acc 85.30%\n",
      "cnn | Epoch 46 | 0m 4s | Train Loss 0.0070 Train Acc 99.76% | Val Loss 1.1213 Val Acc 85.30%\n",
      "cnn | Epoch 47 | 0m 4s | Train Loss 0.0056 Train Acc 99.83% | Val Loss 1.1525 Val Acc 85.20%\n",
      "cnn | Epoch 48 | 0m 4s | Train Loss 0.0061 Train Acc 99.77% | Val Loss 1.2004 Val Acc 85.03%\n",
      "cnn | Epoch 49 | 0m 4s | Train Loss 0.0050 Train Acc 99.89% | Val Loss 1.2311 Val Acc 84.85%\n",
      "cnn | Epoch 50 | 0m 4s | Train Loss 0.0078 Train Acc 99.81% | Val Loss 1.2544 Val Acc 84.64%\n",
      "\n",
      "================================================================================\n",
      "Starting lstm (Adam, 50 epochs)\n",
      "lstm | Epoch 01 | 0m 9s | Train Loss 0.6453 Train Acc 61.23% | Val Loss 0.8099 Val Acc 62.29%\n",
      "lstm | Epoch 02 | 0m 9s | Train Loss 0.5875 Train Acc 69.96% | Val Loss 0.5581 Val Acc 73.76%\n",
      "lstm | Epoch 03 | 0m 10s | Train Loss 0.5224 Train Acc 76.03% | Val Loss 0.5472 Val Acc 72.61%\n",
      "lstm | Epoch 04 | 0m 9s | Train Loss 0.4076 Train Acc 82.69% | Val Loss 0.4270 Val Acc 81.58%\n",
      "lstm | Epoch 05 | 0m 9s | Train Loss 0.3419 Train Acc 86.18% | Val Loss 0.4278 Val Acc 81.89%\n",
      "lstm | Epoch 06 | 0m 9s | Train Loss 0.2898 Train Acc 88.68% | Val Loss 0.3967 Val Acc 84.11%\n",
      "lstm | Epoch 07 | 0m 10s | Train Loss 0.2412 Train Acc 91.04% | Val Loss 0.4221 Val Acc 83.92%\n",
      "lstm | Epoch 08 | 0m 9s | Train Loss 0.1880 Train Acc 93.23% | Val Loss 0.3958 Val Acc 83.94%\n",
      "lstm | Epoch 09 | 0m 9s | Train Loss 0.1547 Train Acc 94.59% | Val Loss 0.4519 Val Acc 82.61%\n",
      "lstm | Epoch 10 | 0m 10s | Train Loss 0.1871 Train Acc 92.68% | Val Loss 0.4901 Val Acc 81.26%\n",
      "lstm | Epoch 11 | 0m 9s | Train Loss 0.1370 Train Acc 95.21% | Val Loss 0.5377 Val Acc 78.69%\n",
      "lstm | Epoch 12 | 0m 9s | Train Loss 0.0842 Train Acc 97.22% | Val Loss 0.5270 Val Acc 85.98%\n",
      "lstm | Epoch 13 | 0m 9s | Train Loss 0.0569 Train Acc 98.21% | Val Loss 0.5686 Val Acc 84.41%\n",
      "lstm | Epoch 14 | 0m 9s | Train Loss 0.0458 Train Acc 98.65% | Val Loss 0.5811 Val Acc 81.90%\n",
      "lstm | Epoch 15 | 0m 10s | Train Loss 0.0560 Train Acc 98.22% | Val Loss 0.6022 Val Acc 85.13%\n",
      "lstm | Epoch 16 | 0m 10s | Train Loss 0.0286 Train Acc 99.18% | Val Loss 0.6489 Val Acc 84.77%\n",
      "lstm | Epoch 17 | 0m 10s | Train Loss 0.0169 Train Acc 99.54% | Val Loss 0.7232 Val Acc 85.90%\n",
      "lstm | Epoch 18 | 0m 10s | Train Loss 0.0277 Train Acc 99.09% | Val Loss 0.6828 Val Acc 85.34%\n",
      "lstm | Epoch 19 | 0m 10s | Train Loss 0.0299 Train Acc 99.05% | Val Loss 0.7067 Val Acc 86.44%\n",
      "lstm | Epoch 20 | 0m 10s | Train Loss 0.0081 Train Acc 99.82% | Val Loss 0.8503 Val Acc 85.43%\n",
      "lstm | Epoch 21 | 0m 10s | Train Loss 0.0031 Train Acc 99.95% | Val Loss 0.9482 Val Acc 85.91%\n",
      "lstm | Epoch 22 | 0m 9s | Train Loss 0.0030 Train Acc 99.92% | Val Loss 0.9626 Val Acc 85.60%\n",
      "lstm | Epoch 23 | 0m 10s | Train Loss 0.0197 Train Acc 99.47% | Val Loss 0.7262 Val Acc 84.98%\n",
      "lstm | Epoch 24 | 0m 10s | Train Loss 0.0168 Train Acc 99.47% | Val Loss 0.8339 Val Acc 85.26%\n",
      "lstm | Epoch 25 | 0m 10s | Train Loss 0.0030 Train Acc 99.93% | Val Loss 0.9525 Val Acc 85.47%\n",
      "lstm | Epoch 26 | 0m 10s | Train Loss 0.0035 Train Acc 99.93% | Val Loss 0.9380 Val Acc 85.95%\n",
      "lstm | Epoch 27 | 0m 10s | Train Loss 0.0132 Train Acc 99.61% | Val Loss 0.9128 Val Acc 84.71%\n",
      "lstm | Epoch 28 | 0m 10s | Train Loss 0.0169 Train Acc 99.46% | Val Loss 0.8031 Val Acc 85.19%\n",
      "lstm | Epoch 29 | 0m 9s | Train Loss 0.0464 Train Acc 98.56% | Val Loss 0.7600 Val Acc 85.83%\n",
      "lstm | Epoch 30 | 0m 10s | Train Loss 0.0129 Train Acc 99.60% | Val Loss 0.8420 Val Acc 84.88%\n",
      "lstm | Epoch 31 | 0m 10s | Train Loss 0.0041 Train Acc 99.91% | Val Loss 0.9423 Val Acc 84.90%\n",
      "lstm | Epoch 32 | 0m 9s | Train Loss 0.0009 Train Acc 99.99% | Val Loss 1.0098 Val Acc 85.66%\n",
      "lstm | Epoch 33 | 0m 10s | Train Loss 0.0004 Train Acc 99.99% | Val Loss 1.0609 Val Acc 85.86%\n",
      "lstm | Epoch 34 | 0m 10s | Train Loss 0.0002 Train Acc 100.00% | Val Loss 1.1056 Val Acc 85.84%\n",
      "lstm | Epoch 35 | 0m 10s | Train Loss 0.0002 Train Acc 100.00% | Val Loss 1.1484 Val Acc 85.93%\n",
      "lstm | Epoch 36 | 0m 10s | Train Loss 0.0002 Train Acc 100.00% | Val Loss 1.1676 Val Acc 85.85%\n",
      "lstm | Epoch 37 | 0m 10s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.2037 Val Acc 85.94%\n",
      "lstm | Epoch 38 | 0m 9s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.2411 Val Acc 86.07%\n",
      "lstm | Epoch 39 | 0m 10s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.2651 Val Acc 85.93%\n",
      "lstm | Epoch 40 | 0m 9s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.2987 Val Acc 85.90%\n",
      "lstm | Epoch 41 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.3210 Val Acc 85.88%\n",
      "lstm | Epoch 42 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.3411 Val Acc 85.92%\n",
      "lstm | Epoch 43 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.3708 Val Acc 85.94%\n",
      "lstm | Epoch 44 | 0m 10s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.3660 Val Acc 85.78%\n",
      "lstm | Epoch 45 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.4033 Val Acc 85.96%\n",
      "lstm | Epoch 46 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.4370 Val Acc 86.10%\n",
      "lstm | Epoch 47 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.4710 Val Acc 85.78%\n",
      "lstm | Epoch 48 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.4925 Val Acc 86.07%\n",
      "lstm | Epoch 49 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.5110 Val Acc 86.09%\n",
      "lstm | Epoch 50 | 0m 9s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.5183 Val Acc 86.15%\n",
      "\n",
      "================================================================================\n",
      "Starting bilstm (Adam, 50 epochs)\n",
      "bilstm | Epoch 01 | 0m 19s | Train Loss 0.6864 Train Acc 55.44% | Val Loss 0.6812 Val Acc 55.21%\n",
      "bilstm | Epoch 02 | 0m 18s | Train Loss 0.6288 Train Acc 64.54% | Val Loss 0.5724 Val Acc 69.69%\n",
      "bilstm | Epoch 03 | 0m 18s | Train Loss 0.5224 Train Acc 74.79% | Val Loss 0.4905 Val Acc 77.01%\n",
      "bilstm | Epoch 04 | 0m 22s | Train Loss 0.3985 Train Acc 82.61% | Val Loss 0.4114 Val Acc 82.07%\n",
      "bilstm | Epoch 05 | 0m 18s | Train Loss 0.3252 Train Acc 86.49% | Val Loss 0.4445 Val Acc 79.80%\n",
      "bilstm | Epoch 06 | 0m 18s | Train Loss 0.3073 Train Acc 87.64% | Val Loss 0.4648 Val Acc 77.45%\n",
      "bilstm | Epoch 07 | 0m 20s | Train Loss 0.2681 Train Acc 89.24% | Val Loss 0.3770 Val Acc 85.56%\n",
      "bilstm | Epoch 08 | 0m 20s | Train Loss 0.1813 Train Acc 93.37% | Val Loss 0.3605 Val Acc 85.69%\n",
      "bilstm | Epoch 09 | 0m 20s | Train Loss 0.1308 Train Acc 95.46% | Val Loss 0.3920 Val Acc 86.82%\n",
      "bilstm | Epoch 10 | 0m 18s | Train Loss 0.0915 Train Acc 97.00% | Val Loss 0.3948 Val Acc 86.15%\n",
      "bilstm | Epoch 11 | 0m 20s | Train Loss 0.0557 Train Acc 98.25% | Val Loss 0.4639 Val Acc 85.99%\n",
      "bilstm | Epoch 12 | 0m 18s | Train Loss 0.0307 Train Acc 99.09% | Val Loss 0.5208 Val Acc 86.71%\n",
      "bilstm | Epoch 13 | 0m 20s | Train Loss 0.0150 Train Acc 99.61% | Val Loss 0.6244 Val Acc 86.09%\n",
      "bilstm | Epoch 14 | 0m 20s | Train Loss 0.0258 Train Acc 99.18% | Val Loss 0.6532 Val Acc 87.03%\n",
      "bilstm | Epoch 15 | 0m 20s | Train Loss 0.0200 Train Acc 99.35% | Val Loss 0.6238 Val Acc 86.83%\n",
      "bilstm | Epoch 16 | 0m 20s | Train Loss 0.0076 Train Acc 99.80% | Val Loss 0.7166 Val Acc 87.15%\n",
      "bilstm | Epoch 17 | 0m 18s | Train Loss 0.0051 Train Acc 99.88% | Val Loss 0.7406 Val Acc 86.85%\n",
      "bilstm | Epoch 18 | 0m 20s | Train Loss 0.0171 Train Acc 99.45% | Val Loss 0.6472 Val Acc 85.79%\n",
      "bilstm | Epoch 19 | 0m 18s | Train Loss 0.0300 Train Acc 99.01% | Val Loss 0.6463 Val Acc 85.17%\n",
      "bilstm | Epoch 20 | 0m 20s | Train Loss 0.0179 Train Acc 99.37% | Val Loss 0.6868 Val Acc 86.49%\n",
      "bilstm | Epoch 21 | 0m 18s | Train Loss 0.0043 Train Acc 99.90% | Val Loss 0.7733 Val Acc 86.71%\n",
      "bilstm | Epoch 22 | 0m 20s | Train Loss 0.0014 Train Acc 99.97% | Val Loss 0.8244 Val Acc 87.01%\n",
      "bilstm | Epoch 23 | 0m 20s | Train Loss 0.0005 Train Acc 100.00% | Val Loss 0.8772 Val Acc 87.27%\n",
      "bilstm | Epoch 24 | 0m 20s | Train Loss 0.0003 Train Acc 100.00% | Val Loss 0.9180 Val Acc 87.30%\n",
      "bilstm | Epoch 25 | 0m 18s | Train Loss 0.0002 Train Acc 100.00% | Val Loss 0.9452 Val Acc 87.15%\n",
      "bilstm | Epoch 26 | 0m 20s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 0.9722 Val Acc 87.23%\n",
      "bilstm | Epoch 27 | 0m 18s | Train Loss 0.0002 Train Acc 99.99% | Val Loss 0.9808 Val Acc 87.36%\n",
      "bilstm | Epoch 28 | 0m 20s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.0123 Val Acc 87.26%\n",
      "bilstm | Epoch 29 | 0m 20s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.0339 Val Acc 87.23%\n",
      "bilstm | Epoch 30 | 0m 20s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.0667 Val Acc 87.31%\n",
      "bilstm | Epoch 31 | 0m 18s | Train Loss 0.0001 Train Acc 100.00% | Val Loss 1.0715 Val Acc 87.32%\n",
      "bilstm | Epoch 32 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.1103 Val Acc 87.24%\n",
      "bilstm | Epoch 33 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.1112 Val Acc 87.30%\n",
      "bilstm | Epoch 34 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.1481 Val Acc 87.35%\n",
      "bilstm | Epoch 35 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.1586 Val Acc 87.24%\n",
      "bilstm | Epoch 36 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.1839 Val Acc 87.34%\n",
      "bilstm | Epoch 37 | 0m 19s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.1608 Val Acc 87.07%\n",
      "bilstm | Epoch 38 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.2127 Val Acc 87.28%\n",
      "bilstm | Epoch 39 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.2176 Val Acc 87.19%\n",
      "bilstm | Epoch 40 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.2575 Val Acc 87.14%\n",
      "bilstm | Epoch 41 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.2692 Val Acc 87.19%\n",
      "bilstm | Epoch 42 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.2777 Val Acc 87.26%\n",
      "bilstm | Epoch 43 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.3055 Val Acc 87.10%\n",
      "bilstm | Epoch 44 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.3288 Val Acc 87.20%\n",
      "bilstm | Epoch 45 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.3455 Val Acc 87.14%\n",
      "bilstm | Epoch 46 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.3619 Val Acc 87.05%\n",
      "bilstm | Epoch 47 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.3817 Val Acc 87.27%\n",
      "bilstm | Epoch 48 | 0m 20s | Train Loss 0.0000 Train Acc 100.00% | Val Loss 1.4152 Val Acc 87.30%\n",
      "bilstm | Epoch 49 | 0m 20s | Train Loss 0.0430 Train Acc 98.55% | Val Loss 0.6575 Val Acc 86.18%\n",
      "bilstm | Epoch 50 | 0m 18s | Train Loss 0.0187 Train Acc 99.34% | Val Loss 0.7430 Val Acc 86.56%\n",
      "All models trained and per-epoch checkpoints saved in models/. Histories saved in history/.\n"
     ]
    }
   ],
   "source": [
    "# Build models and train (50 epochs, Adam)\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMB_DIM = 100\n",
    "HID_DIM = 256\n",
    "OUT_DIM = 1\n",
    "\n",
    "models = build_models(INPUT_DIM, EMBEDDING_DIM=EMB_DIM, HIDDEN_DIM=HID_DIM, OUTPUT_DIM=OUT_DIM)\n",
    "\n",
    "all_histories = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'rnn':\n",
    "        continue\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Starting {name} (Adam, 50 epochs)\")\n",
    "    history, _ = run_and_record(name, model, train_iterator, valid_iterator, n_epochs=50, lr=1e-3, output_prefix=\"results\")\n",
    "    all_histories[name] = history\n",
    "\n",
    "with open(\"history/results-all-models.json\", \"w\") as f:\n",
    "    json.dump(all_histories, f, indent=2)\n",
    "\n",
    "print(\"All models trained and per-epoch checkpoints saved in models/. Histories saved in history/.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2a67b",
   "metadata": {},
   "source": [
    "### Load and View Results\n",
    "Created two dataframes:\n",
    "1. `val_df`: The accuracies of the model on validation set at given epoch.\n",
    "2. `test_df`: results of the best model of each type, after running on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76cd4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"models\"\n",
    "HISTORY_DIR = RESULTS_DIR = \"history\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "VALID_EPOCHS = [5, 10, 20, 50] # Load validation results on epoch 5, 10, 20 and 50\n",
    "\n",
    "def load_validation_from_history(model_name):\n",
    "    fn = os.path.join(HISTORY_DIR, f\"results-{model_name}-history.json\")\n",
    "    if not os.path.exists(fn):\n",
    "        return None\n",
    "    with open(fn, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    epoch_map = {}\n",
    "    if isinstance(data, dict):\n",
    "        entries = [data]\n",
    "    else:\n",
    "        entries = data\n",
    "    for entry in entries:\n",
    "        if not isinstance(entry, dict):\n",
    "            continue\n",
    "        if \"epoch\" in entry and (\"valid_acc\" in entry or \"valid_accuracy\" in entry):\n",
    "            acc_key = \"valid_acc\" if \"valid_acc\" in entry else \"valid_accuracy\"\n",
    "            try:\n",
    "                epoch_map[int(entry[\"epoch\"])] = float(entry[acc_key])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return epoch_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057714f9",
   "metadata": {},
   "source": [
    "Load the best models of each type and get results on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c545aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EPOCHS = [\"best\"]\n",
    "\n",
    "def evaluate_checkpoints_on_test(models_list, test_iterator, epochs_to_check=TEST_EPOCHS, prefix=\"results\"):\n",
    "    results_by_model = {}\n",
    "    for model_name in models_list:\n",
    "        model_results = []\n",
    "        for epoch_label in epochs_to_check:\n",
    "            # build candidate paths\n",
    "            if epoch_label == \"best\":\n",
    "                candidates = [\n",
    "                    os.path.join(MODELS_DIR, f\"{prefix}-{model_name}-best.pt\"),\n",
    "                    os.path.join(MODELS_DIR, f\"{model_name}-best.pt\"),\n",
    "                ]\n",
    "            else:\n",
    "                candidates = [\n",
    "                    os.path.join(MODELS_DIR, f\"{prefix}-{model_name}-epoch{epoch_label}.pt\"),\n",
    "                    os.path.join(MODELS_DIR, f\"{model_name}-epoch{epoch_label}.pt\"),\n",
    "                ]\n",
    "            ckpt_path = next((p for p in candidates if os.path.exists(p)), None)\n",
    "            if ckpt_path is None:\n",
    "                model_results.append((str(epoch_label), None))\n",
    "                continue\n",
    "\n",
    "            # instantiate fresh model and load state\n",
    "            models_dict = build_models(len(TEXT.vocab))\n",
    "            if model_name.startswith('rnn'): model = models_dict['rnn']\n",
    "            else: model = models_dict[model_name]\n",
    "\n",
    "            print(model_name, ckpt_path)\n",
    "            state = torch.load(ckpt_path, map_location=DEVICE)\n",
    "            # handle saved dicts that wrap state dicts\n",
    "            if isinstance(state, dict) and \"state_dict\" in state:\n",
    "                state = state[\"state_dict\"]\n",
    "            model.load_state_dict(state)\n",
    "            model = model.to(DEVICE)\n",
    "            criterion = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "            loss, acc = evaluate(model, device_iterator(test_iterator, DEVICE), criterion)\n",
    "            if isinstance(acc, torch.Tensor):\n",
    "                acc = acc.item()\n",
    "            model_results.append((str(epoch_label), float(acc)))\n",
    "        results_by_model[model_name] = model_results\n",
    "\n",
    "    # save JSON summary\n",
    "    out = {m: [{\"epoch\": e, \"test_acc\": (None if a is None else a)} for e, a in lst] for m, lst in results_by_model.items()}\n",
    "    with open(os.path.join(RESULTS_DIR, \"results-test-summary.json\"), \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    return results_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8ac1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build validation table from histories\n",
    "def build_validation_table(models_list, epochs_list=VALID_EPOCHS):\n",
    "    rows = {}\n",
    "    for m in models_list:\n",
    "        hist = load_validation_from_history(m)\n",
    "        if hist is None:\n",
    "            rows[m] = {e: float(\"nan\") for e in epochs_list}\n",
    "            continue\n",
    "        row = {}\n",
    "        for e in epochs_list:\n",
    "            if e in hist:\n",
    "                row[e] = hist[e]\n",
    "            else:\n",
    "                row[e] = float(\"nan\")\n",
    "        rows[m] = row\n",
    "    df = pd.DataFrame.from_dict(rows, orient=\"index\", columns=epochs_list)\n",
    "    df.index.name = \"model\"\n",
    "    df.to_csv(os.path.join(RESULTS_DIR, \"validation-accuracies-by-epoch.csv\"))\n",
    "    with open(os.path.join(RESULTS_DIR, \"validation-accuracies-by-epoch.md\"), \"w\") as f:\n",
    "        f.write(df.to_markdown())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f5c3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test table from results_by_model\n",
    "def build_test_table(results_by_model, epochs_list=TEST_EPOCHS):\n",
    "    rows = {}\n",
    "    for m, lst in results_by_model.items():\n",
    "        mapping = {e: a for e, a in lst}\n",
    "        rows[m] = {str(e): (float(mapping.get(str(e))) if mapping.get(str(e)) is not None else float(\"nan\")) for e in epochs_list}\n",
    "    df = pd.DataFrame.from_dict(rows, orient=\"index\", columns=[str(e) for e in epochs_list])\n",
    "    df.index.name = \"model\"\n",
    "    df.to_csv(os.path.join(RESULTS_DIR, \"test-accuracies-by-epoch.csv\"))\n",
    "    with open(os.path.join(RESULTS_DIR, \"test-accuracies-by-epoch.md\"), \"w\") as f:\n",
    "        f.write(df.to_markdown())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86479c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff1 models\\results-ff1-best.pt\n",
      "ff2 models\\results-ff2-best.pt\n",
      "ff3 models\\results-ff3-best.pt\n",
      "cnn models\\results-cnn-best.pt\n",
      "lstm models\\results-lstm-best.pt\n",
      "bilstm models\\results-bilstm-best.pt\n",
      "rnn-Adagrad models\\results-rnn-Adagrad-best.pt\n",
      "rnn-SGD models\\results-rnn-SGD-best.pt\n",
      "rnn-Adam-e5 models\\results-rnn-Adam-e5-best.pt\n",
      "rnn-Adam-e10 models\\results-rnn-Adam-e10-best.pt\n",
      "rnn-Adam-e20 models\\results-rnn-Adam-e20-best.pt\n",
      "rnn-Adam-e50 models\\results-rnn-Adam-e50-best.pt\n"
     ]
    }
   ],
   "source": [
    "models_list = [\n",
    " 'ff1',\n",
    " 'ff2',\n",
    " 'ff3',\n",
    " 'cnn',\n",
    " 'lstm',\n",
    " 'bilstm',\n",
    " 'rnn-Adagrad', \n",
    " 'rnn-SGD',\n",
    " 'rnn-Adam-e5',\n",
    " 'rnn-Adam-e10',\n",
    " 'rnn-Adam-e20',\n",
    " 'rnn-Adam-e50'\n",
    "]\n",
    "\n",
    "\n",
    "test_results = evaluate_checkpoints_on_test(models_list, test_iterator, epochs_to_check=TEST_EPOCHS, prefix=\"results\")\n",
    "val_df = build_validation_table(models_list, epochs_list=VALID_EPOCHS)\n",
    "test_df = build_test_table(test_results, epochs_list=TEST_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0bd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3098b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ff1</th>\n",
       "      <td>0.878178</td>\n",
       "      <td>0.879635</td>\n",
       "      <td>0.878487</td>\n",
       "      <td>0.873014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff2</th>\n",
       "      <td>0.873985</td>\n",
       "      <td>0.875221</td>\n",
       "      <td>0.874382</td>\n",
       "      <td>0.873720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff3</th>\n",
       "      <td>0.876368</td>\n",
       "      <td>0.873985</td>\n",
       "      <td>0.872925</td>\n",
       "      <td>0.872528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.856109</td>\n",
       "      <td>0.883121</td>\n",
       "      <td>0.871337</td>\n",
       "      <td>0.846354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>0.818900</td>\n",
       "      <td>0.812588</td>\n",
       "      <td>0.854299</td>\n",
       "      <td>0.861538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstm</th>\n",
       "      <td>0.798023</td>\n",
       "      <td>0.861494</td>\n",
       "      <td>0.864936</td>\n",
       "      <td>0.865643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adagrad</th>\n",
       "      <td>0.659031</td>\n",
       "      <td>0.672007</td>\n",
       "      <td>0.706082</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-SGD</th>\n",
       "      <td>0.534428</td>\n",
       "      <td>0.631532</td>\n",
       "      <td>0.764786</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adam-e5</th>\n",
       "      <td>0.681806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adam-e10</th>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.683351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adam-e20</th>\n",
       "      <td>0.667196</td>\n",
       "      <td>0.760814</td>\n",
       "      <td>0.684896</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adam-e50</th>\n",
       "      <td>0.655456</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>0.584878</td>\n",
       "      <td>0.676333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    5         10        20        50\n",
       "model                                               \n",
       "ff1           0.878178  0.879635  0.878487  0.873014\n",
       "ff2           0.873985  0.875221  0.874382  0.873720\n",
       "ff3           0.876368  0.873985  0.872925  0.872528\n",
       "cnn           0.856109  0.883121  0.871337  0.846354\n",
       "lstm          0.818900  0.812588  0.854299  0.861538\n",
       "bilstm        0.798023  0.861494  0.864936  0.865643\n",
       "rnn-Adagrad   0.659031  0.672007  0.706082       NaN\n",
       "rnn-SGD       0.534428  0.631532  0.764786       NaN\n",
       "rnn-Adam-e5   0.681806       NaN       NaN       NaN\n",
       "rnn-Adam-e10  0.610169  0.683351       NaN       NaN\n",
       "rnn-Adam-e20  0.667196  0.760814  0.684896       NaN\n",
       "rnn-Adam-e50  0.655456  0.570312  0.584878  0.676333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b58fed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ff1</th>\n",
       "      <td>0.864003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff2</th>\n",
       "      <td>0.858488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff3</th>\n",
       "      <td>0.858967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.877126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>0.827126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilstm</th>\n",
       "      <td>0.848857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adagrad</th>\n",
       "      <td>0.663059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-SGD</th>\n",
       "      <td>0.736165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adam-e5</th>\n",
       "      <td>0.695836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adam-e10</th>\n",
       "      <td>0.697642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adam-e20</th>\n",
       "      <td>0.766152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-Adam-e50</th>\n",
       "      <td>0.713539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  best\n",
       "model                 \n",
       "ff1           0.864003\n",
       "ff2           0.858488\n",
       "ff3           0.858967\n",
       "cnn           0.877126\n",
       "lstm          0.827126\n",
       "bilstm        0.848857\n",
       "rnn-Adagrad   0.663059\n",
       "rnn-SGD       0.736165\n",
       "rnn-Adam-e5   0.695836\n",
       "rnn-Adam-e10  0.697642\n",
       "rnn-Adam-e20  0.766152\n",
       "rnn-Adam-e50  0.713539"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
